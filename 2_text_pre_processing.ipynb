{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Proceesing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our approach is to explore prominent themes in the tweets, we have consciously decided to not purge duplicates.\n",
    "The presence of duplicates (or retweets here) indicate that multiple people agree or share the opinion.\n",
    "\n",
    "In data processing we first performed tokenization. Tokenization is the process of breaking a stream of textual content into \n",
    "words, themes, symbols or some meaningful elements called tokens. Here we generated unigrams, bigrams and trigrams for each and \n",
    "every row or tweet in the data. Then we created three new columns in the dataset containing these unigrams, bigrams and trigrams of the tweets present in the respective rows. \n",
    "The idea behind this tokenization was to better understand the tweets. We also performed lemmatization on the tokens generated \n",
    "from the tweets and created a new column to store this lemmatized words.\n",
    "\n",
    "Finally we performed Naive Senyiment analysis and added a new column where we tagged each tweet as positive or negetive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id           created_at              user_id  \\\n",
      "0  1069693113013321728  2018-12-03 20:41:26            342431891   \n",
      "1  1069692711077326848  2018-12-03 20:39:50             50577341   \n",
      "2  1069691473015107584  2018-12-03 20:34:55  1002325987919474693   \n",
      "3  1069689874192711680  2018-12-03 20:28:34            581308005   \n",
      "4  1069689321505153030  2018-12-03 20:26:22            239173653   \n",
      "\n",
      "       screen_name                                               text  \n",
      "0          JKNACK3  b'So just saw my friend\\xe2\\x80\\x99s #iPhoneXS...  \n",
      "1   DudeFaceGeoffy  b'My first selfie on my new iphone xs #Selfie ...  \n",
      "2  MobilizeStories  b'RT @dharavisual: Shot on iPhone XS Max! #app...  \n",
      "3     hagiofficial  b'RT @pschiller: Play To Win. \\n#Apple #iPhone...  \n",
      "4    ToddPatrick51  b\"@VerizonSupport I have buyer's remorse with ...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10057 entries, 0 to 10056\n",
      "Data columns (total 5 columns):\n",
      "id             10057 non-null int64\n",
      "created_at     10057 non-null datetime64[ns]\n",
      "user_id        10057 non-null int64\n",
      "screen_name    10057 non-null object\n",
      "text           10057 non-null object\n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 392.9+ KB\n",
      "None\n",
      "Now we print the head of the modified data frame with all the new columns\n",
      "                    id          created_at              user_id  \\\n",
      "0  1069693113013321728 2018-12-03 20:41:26            342431891   \n",
      "1  1069692711077326848 2018-12-03 20:39:50             50577341   \n",
      "2  1069691473015107584 2018-12-03 20:34:55  1002325987919474693   \n",
      "3  1069689874192711680 2018-12-03 20:28:34            581308005   \n",
      "4  1069689321505153030 2018-12-03 20:26:22            239173653   \n",
      "\n",
      "       screen_name                                               text  \\\n",
      "0          JKNACK3  so just saw my friend’s #iphonexs camera glass...   \n",
      "1   DudeFaceGeoffy  my first selfie on my new iphone xs #selfie #s...   \n",
      "2  MobilizeStories   @dharavisual: shot on iphone xs max! #apple #...   \n",
      "3     hagiofficial   @pschiller: play to win. \\n#apple #iphonexr #...   \n",
      "4    ToddPatrick51  @verizonsupport i have buyer's remorse with th...   \n",
      "\n",
      "                                            unigrams  \\\n",
      "0  [saw, friend, ’, iphonexs, camera, glass, crac...   \n",
      "1  [first, selfie, new, iphone, xs, selfie, shame...   \n",
      "2  [dharavisual, shot, iphone, xs, max, apple, ip...   \n",
      "3  [pschiller, play, win, apple, iphonexr, iphone...   \n",
      "4  [verizonsupport, buyer, remorse, apple, iphone...   \n",
      "\n",
      "                                             bigrams  \\\n",
      "0  [(saw, friend), (friend, ’), (’, iphonexs), (i...   \n",
      "1  [(first, selfie), (selfie, new), (new, iphone)...   \n",
      "2  [(dharavisual, shot), (shot, iphone), (iphone,...   \n",
      "3  [(pschiller, play), (play, win), (win, apple),...   \n",
      "4  [(verizonsupport, buyer), (buyer, remorse), (r...   \n",
      "\n",
      "                                            trigrams  \\\n",
      "0  [(saw, friend, ’), (friend, ’, iphonexs), (’, ...   \n",
      "1  [(first, selfie, new), (selfie, new, iphone), ...   \n",
      "2  [(dharavisual, shot, iphone), (shot, iphone, x...   \n",
      "3  [(pschiller, play, win), (play, win, apple), (...   \n",
      "4  [(verizonsupport, buyer, remorse), (buyer, rem...   \n",
      "\n",
      "                                    lemmatized_words  Positive_count  \\\n",
      "0  [saw, friend, ’, iphonexs, camera, glass, crac...               0   \n",
      "1  [first, selfie, new, iphone, xs, selfie, shame...               0   \n",
      "2  [dharavisual, shot, iphone, x, max, apple, iph...               0   \n",
      "3  [pschiller, play, win, apple, iphonexr, iphone...               1   \n",
      "4  [verizonsupport, buyer, 's, remorse, apple, ip...               0   \n",
      "\n",
      "   Negative_count More_positive_than_Negative  \n",
      "0               2                         Neg  \n",
      "1               0                         Neg  \n",
      "2               0                         Neg  \n",
      "3               0                         Pos  \n",
      "4               1                         Neg  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast   \n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# change this to work with a different local file\n",
    "TEXT_FILE = r\"data\\iPhoneXS_twitter.csv\"\n",
    "EDA_FILE = r\"data\\iPhoneXS_twitter_eda.csv\"\n",
    "\n",
    "def get_unigrams(text, extra_stop_words=None):  \n",
    "    stop = stopwords.words()\n",
    "    if extra_stop_words is not None:\n",
    "        stop += extra_stop_words\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "    \n",
    "    #removing stopwords\n",
    "    tokens = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    #remove empty tokens\n",
    "    tokens=[token.strip() for token in tokens if token.strip() != '']\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "#function used for wordnet tagging\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "\n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "\n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_words(text, extra_stop_words=None):\n",
    "    stop_words = stopwords.words('english')\n",
    "    if extra_stop_words is not None:\n",
    "        stop_words += extra_stop_words\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize all words in the tagged tokens removing stopwords & punctuation\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "            (word, get_wordnet_pos(tag)) \\\n",
    "            # tagged_tokens is a list of tuples (word, tag)\n",
    "            for (word, tag) in tagged_tokens \\\n",
    "            # remove stop words\n",
    "            if word not in stop_words and \\\n",
    "            word not in string.punctuation]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "def Posgrams(text):  \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "    \n",
    "    #removing stopwords\n",
    "    stop = stopwords.words()\n",
    "    tokens = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    #remove empty tokens\n",
    "    tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "    \n",
    "    with open(r\"A:\\BIA 660 - Web Mining\\project\\product-buzz-analysis\\scraper\\positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "    \n",
    "    positive_tokens=[token for token in tokens if token in positive_words]\n",
    "    \n",
    "    return len(positive_tokens)\n",
    "    \n",
    "def Neggrams(text):  \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "    \n",
    "    #removing stopwords\n",
    "    stop = stopwords.words()\n",
    "    tokens = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    #remove empty tokens\n",
    "    tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "    \n",
    "    with open(r\"A:\\BIA 660 - Web Mining\\project\\product-buzz-analysis\\scraper\\negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "    \n",
    "    negative_tokens=[token for token in tokens if token in negative_words]\n",
    "    \n",
    "    return len(negative_tokens)\n",
    "\n",
    "def comment_data_eda(comments_file_path):\n",
    "    #reading the data into a pandas dataframe\n",
    "    comments_df = pd.read_csv(comments_file_path)\n",
    "    #having a first look at all the columns before going ahead\n",
    "    print(comments_df.head())\n",
    "    #converting the created_at column to pandas datetime\n",
    "    comments_df['created_at'] = pd.to_datetime(comments_df.created_at)\n",
    "    print(comments_df.info())\n",
    "    \n",
    "    # decode byte string to utf-8\n",
    "    comments_df['text'] = comments_df['text'].apply(ast.literal_eval).str.decode('utf-8')\n",
    "    # remove 'RT' which denotes re-tweet\n",
    "    comments_df['text'] = comments_df['text'].apply(lambda x: x.replace('RT', ''))\n",
    "    #convert all the text to lower case for further text mining\n",
    "    comments_df['text'] = comments_df['text'].str.lower()\n",
    "    \n",
    "    #make new columns to get the unigrams, bigrams, trigrams from each comments\n",
    "    #making three new columns for unigrams, bigrams and trigrams\n",
    "    comments_df['unigrams'] = comments_df['text'].apply(get_unigrams)\n",
    "    comments_df['bigrams'] = comments_df['unigrams'].apply(lambda x: list(nltk.bigrams(x))) \n",
    "    comments_df['trigrams'] = comments_df['unigrams'].apply(lambda x: list(nltk.trigrams(x))) \n",
    "    \n",
    "    #Now we will lemmatize each comment and keep the lemmatized output in a new column\n",
    "    #make a new column to store the lemmatized words\n",
    "    comments_df['lemmatized_words'] = comments_df['text'].apply(lemmatize_words)\n",
    "\n",
    "    # Basic sentiment analysis by looking at positive and negative word count\n",
    "    comments_df['Positive_count'] = comments_df['text'].apply(Posgrams)\n",
    "    comments_df['Negative_count'] = comments_df['text'].apply(Neggrams)\n",
    "    comments_df['More_positive_than_Negative']='Neg'\n",
    "    comments_df.loc[comments_df.Positive_count>comments_df.Negative_count,'More_positive_than_Negative']='Pos'\n",
    "\n",
    "    return comments_df                                                      \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #first let's have a look at the dataframe\n",
    "    return_object = comment_data_eda(comments_file_path = os.path.abspath(TEXT_FILE))\n",
    "    \n",
    "    print(\"Now we print the head of the modified data frame with all the new columns\") \n",
    "    print(return_object.head())\n",
    "\n",
    "    return_object.to_csv(os.path.abspath(EDA_FILE), sep=',', encoding='utf-8', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
